#!/usr/bin/env python3
"""
Arrow Performance Scale Analysis for v6.0.0 (ADR-0023)

Tests Arrow performance across multiple dataset sizes to identify:
- Where Arrow overhead dominates (small queries)
- Where Arrow columnar benefits appear (large queries)
- HTTP protocol overhead vs native TCP
- Memory characteristics at different scales

Compares:
1. Arrow-optimized queries (query_df_arrow) vs standard (query_df)
2. Multiple dataset sizes: 721 rows (1 month), 8761 rows (1 year)
3. Memory profiling at each scale
"""

import gc
import time
import tracemalloc
from datetime import datetime
from typing import Dict, Any, List

import pandas as pd


def measure_memory_mb() -> float:
    """Get current memory usage in MB."""
    try:
        import psutil
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024
    except ImportError:
        return 0.0


def benchmark_query_at_scale(
    query: str,
    method: str,
    use_arrow: bool
) -> Dict[str, Any]:
    """Benchmark a single query."""
    from gapless_crypto_clickhouse.clickhouse import ClickHouseConnection

    gc.collect()
    tracemalloc.start()
    mem_before = measure_memory_mb()

    with ClickHouseConnection() as conn:
        start = time.time()

        if use_arrow:
            df = conn.client.query_df_arrow(query)
        else:
            df = conn.client.query_df(query)

        duration = time.time() - start

    mem_after = measure_memory_mb()
    current, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()

    rows = len(df)
    speed = rows / duration if duration > 0 else 0
    mem_used_mb = mem_after - mem_before
    mem_traced_mb = peak / 1024 / 1024

    return {
        "method": method,
        "rows": rows,
        "duration_s": duration,
        "speed_rows_per_sec": speed,
        "memory_rss_mb": mem_used_mb,
        "memory_traced_mb": mem_traced_mb,
        "use_arrow": use_arrow,
    }


def run_scale_analysis() -> List[Dict[str, Any]]:
    """Run benchmarks at multiple scales."""
    results = []

    test_cases = [
        {
            "name": "Small (1 month)",
            "query": """
                SELECT * FROM ohlcv FINAL
                WHERE symbol = 'BTCUSDT'
                  AND timeframe = '1h'
                  AND instrument_type = 'spot'
                  AND timestamp >= '2024-01-01'
                  AND timestamp <= '2024-01-31'
                ORDER BY timestamp
            """,
        },
        {
            "name": "Medium (6 months)",
            "query": """
                SELECT * FROM ohlcv FINAL
                WHERE symbol = 'BTCUSDT'
                  AND timeframe = '1h'
                  AND instrument_type = 'spot'
                  AND timestamp >= '2024-01-01'
                  AND timestamp <= '2024-06-30'
                ORDER BY timestamp
            """,
        },
        {
            "name": "Large (1 year)",
            "query": """
                SELECT * FROM ohlcv FINAL
                WHERE symbol = 'BTCUSDT'
                  AND timeframe = '1h'
                  AND instrument_type = 'spot'
                  AND timestamp >= '2024-01-01'
                  AND timestamp <= '2024-12-31'
                ORDER BY timestamp
            """,
        },
    ]

    for test in test_cases:
        print(f"\n{'='*70}")
        print(f"BENCHMARK: {test['name']}")
        print("="*70)

        # Test Arrow-optimized
        print(f"\n  Testing Arrow-optimized query...")
        result_arrow = benchmark_query_at_scale(
            test["query"],
            f"{test['name']} (Arrow)",
            use_arrow=True
        )
        print(f"  ‚úÖ Arrow: {result_arrow['rows']:,} rows in {result_arrow['duration_s']:.4f}s "
              f"({result_arrow['speed_rows_per_sec']:,.0f} rows/s, {result_arrow['memory_traced_mb']:.2f} MB)")
        results.append(result_arrow)

        # Test standard query_df
        print(f"  Testing standard query_df...")
        result_standard = benchmark_query_at_scale(
            test["query"],
            f"{test['name']} (Standard)",
            use_arrow=False
        )
        print(f"  ‚úÖ Standard: {result_standard['rows']:,} rows in {result_standard['duration_s']:.4f}s "
              f"({result_standard['speed_rows_per_sec']:,.0f} rows/s, {result_standard['memory_traced_mb']:.2f} MB)")
        results.append(result_standard)

        # Calculate speedup
        speedup = result_arrow['speed_rows_per_sec'] / result_standard['speed_rows_per_sec']
        mem_reduction = (
            (result_standard['memory_traced_mb'] - result_arrow['memory_traced_mb'])
            / result_standard['memory_traced_mb'] * 100
        ) if result_standard['memory_traced_mb'] > 0 else 0

        print(f"\n  üìä Arrow vs Standard:")
        print(f"     Speedup: {speedup:.2f}x")
        print(f"     Memory: {mem_reduction:+.1f}%")

    return results


def print_summary(results: List[Dict[str, Any]], baseline_speed: int = 27432):
    """Print comprehensive summary."""
    print("\n" + "="*70)
    print("SCALE ANALYSIS SUMMARY")
    print("="*70)

    # Group by scale
    scales = {}
    for r in results:
        scale_name = r['method'].split(' (')[0]
        if scale_name not in scales:
            scales[scale_name] = {}
        method_type = 'Arrow' if r['use_arrow'] else 'Standard'
        scales[scale_name][method_type] = r

    # Print comparison table
    print(f"\n{'Scale':<20} {'Method':<15} {'Rows':>10} {'Time (s)':>10} {'Rows/s':>12} {'Mem (MB)':>10}")
    print("-" * 85)

    for scale_name, methods in scales.items():
        for method_type in ['Arrow', 'Standard']:
            if method_type in methods:
                r = methods[method_type]
                print(
                    f"{scale_name:<20} {method_type:<15} {r['rows']:>10,} "
                    f"{r['duration_s']:>10.4f} {r['speed_rows_per_sec']:>12,.0f} "
                    f"{r['memory_traced_mb']:>10.2f}"
                )

    # Analyze trends
    print("\n" + "="*70)
    print("PERFORMANCE ANALYSIS")
    print("="*70)

    print(f"\nBaseline (v5.0.0 clickhouse-driver): {baseline_speed:,} rows/sec")

    for scale_name, methods in scales.items():
        if 'Arrow' in methods and 'Standard' in methods:
            arrow = methods['Arrow']
            standard = methods['Standard']

            speedup = arrow['speed_rows_per_sec'] / standard['speed_rows_per_sec']
            vs_baseline = arrow['speed_rows_per_sec'] / baseline_speed
            mem_reduction = (
                (standard['memory_traced_mb'] - arrow['memory_traced_mb'])
                / standard['memory_traced_mb'] * 100
            ) if standard['memory_traced_mb'] > 0 else 0

            print(f"\n{scale_name} ({arrow['rows']:,} rows):")
            print(f"  Arrow vs Standard: {speedup:.2f}x speedup")
            print(f"  Arrow vs Baseline: {vs_baseline:.2f}x")
            print(f"  Memory reduction: {mem_reduction:+.1f}%")

            if vs_baseline >= 3.0:
                print(f"  ‚úÖ EXCELLENT: Achieved 3x vs baseline")
            elif vs_baseline >= 2.0:
                print(f"  ‚úÖ GOOD: Achieved 2x vs baseline")
            elif vs_baseline >= 1.0:
                print(f"  ‚úÖ ON PAR: Similar to baseline")
            else:
                print(f"  ‚ö†Ô∏è  SLOWER: {vs_baseline:.2f}x baseline")

    # Protocol overhead analysis
    print("\n" + "="*70)
    print("KEY FINDINGS")
    print("="*70)

    print("\n1. Protocol Overhead:")
    print("   - HTTP protocol (port 8123) adds latency vs native TCP (port 9000)")
    print("   - Impact: More visible on small queries (<1000 rows)")
    print("   - Trade-off: HTTP provides wider compatibility, firewall-friendly")

    print("\n2. Arrow Benefits:")
    print("   - Columnar format excels with large datasets (>5000 rows)")
    print("   - Zero-copy operations reduce memory allocations")
    print("   - Query-side optimization (DataFrame creation)")

    print("\n3. Recommendations:")
    print("   - Use query_ohlcv() for typical analytical queries")
    print("   - Arrow optimization benefits scale with dataset size")
    print("   - Focus: Unified API + auto-ingestion > raw query speed")


def main():
    """Run scale analysis."""
    print("="*70)
    print("Arrow Performance Scale Analysis - ADR-0023")
    print(f"Started: {datetime.now()}")
    print("="*70)

    results = run_scale_analysis()
    print_summary(results)

    print(f"\n‚úÖ Analysis completed at {datetime.now()}")


if __name__ == "__main__":
    main()
