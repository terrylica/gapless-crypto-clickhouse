name: CI

on:
  push:
    branches:
      - main
      - develop
  pull_request:
    branches:
      - main

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

env:
  UV_SYSTEM_PYTHON: 1

jobs:
  test-fast:
    name: Fast Tests (Static + Unit + Integration)
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.12", "3.13"]
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Install dependencies
        run: uv sync

      - name: Run Ruff linting
        run: uv run ruff check .

      - name: Run Ruff formatting check
        run: uv run ruff format --check .

      - name: Run unit and integration tests
        run: uv run pytest -v --cov=src --cov-report=xml --cov-report=term -m "not e2e and not slow"

      - name: Upload coverage
        if: matrix.python-version == '3.12'
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage.xml
          flags: unittests

  test-e2e:
    name: E2E Tests (Playwright + Visual Regression)
    runs-on: ubuntu-latest
    timeout-minutes: 15
    services:
      clickhouse:
        image: clickhouse/clickhouse-server:24.11
        ports:
          - 8123:8123
          - 9000:9000
        options: >-
          --health-cmd="wget --no-verbose --tries=1 --spider http://localhost:8123/ping || exit 1"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=5
      ch-ui:
        image: ghcr.io/caioricciuti/ch-ui:latest
        ports:
          - 5521:5521
        env:
          VITE_CLICKHOUSE_URL: http://clickhouse:8123
        options: >-
          --health-cmd="wget --no-verbose --tries=1 --spider http://localhost:5521/ || exit 1"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=5
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Install dependencies
        run: uv sync

      - name: Cache Playwright browsers
        uses: actions/cache@v4
        id: playwright-cache
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-playwright-${{ hashFiles('**/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-playwright-

      - name: Install Playwright browsers
        if: steps.playwright-cache.outputs.cache-hit != 'true'
        run: uv run playwright install chromium --with-deps

      - name: Install Playwright dependencies only
        if: steps.playwright-cache.outputs.cache-hit == 'true'
        run: uv run playwright install-deps chromium

      - name: Wait for ClickHouse
        run: |
          timeout 60 bash -c 'until wget --spider http://localhost:8123/ping; do sleep 2; done'

      - name: Wait for CH-UI
        run: |
          timeout 60 bash -c 'until wget --spider http://localhost:5521/; do sleep 2; done'

      - name: Populate ClickHouse with test data
        run: |
          # Create schema and load test data for slow/integration tests
          uv run python -c "
          from src.gapless_crypto_clickhouse.clickhouse.connection import ClickHouseConnection
          from src.gapless_crypto_clickhouse.clickhouse.schema import create_schema

          with ClickHouseConnection() as conn:
              create_schema(conn.client)
              print('âœ… Schema created successfully')
          "

      - name: Run integration tests (@pytest.mark.slow)
        run: |
          # Run integration tests that require ClickHouse database
          # Includes Arrow equivalence, deduplication, multi-symbol isolation, etc.
          uv run pytest -v -m "slow" --tb=short

      - name: Run E2E tests with visual regression
        run: |
          # Run all 12 E2E tests (6 CH-UI + 6 ClickHouse Play)
          # Playwright automatically compares screenshots to baselines in tests/e2e/**/*.png
          # Visual regression enabled via expect(page).to_have_screenshot()
          uv run pytest tests/e2e/ -v --screenshot=only-on-failure --tracing=retain-on-failure

      - name: Upload test artifacts
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-artifacts-${{ github.run_id }}
          path: |
            tmp/validation-artifacts/screenshots/
            tests/e2e/**/*-diff.png
            tests/e2e/**/*-actual.png
            test-results/
          if-no-files-found: warn
          retention-days: 7

      - name: Comment on PR with visual regression results
        if: failure() && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            // Check if diff images exist
            const diffDir = 'tests/e2e/';
            let diffCount = 0;
            try {
              const files = fs.readdirSync(diffDir, { recursive: true });
              diffCount = files.filter(f => f.endsWith('-diff.png')).length;
            } catch (e) {
              console.log('No diff directory found');
            }

            if (diffCount > 0) {
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: `## âš ï¸ Visual Regression Detected\n\n${diffCount} screenshot(s) differ from baselines.\n\n**Action Required:** Review diff images in [E2E artifacts](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}).\n\nIf changes are intentional, update baselines:\n\`\`\`bash\nuv run pytest tests/e2e/ --update-snapshots\ngit add tests/e2e/**/*.png\ngit commit -m "chore: update E2E screenshot baselines"\n\`\`\``
              });
            }

  benchmark:
    name: Performance Benchmarks (Informational)
    runs-on: ubuntu-latest
    timeout-minutes: 20
    continue-on-error: true  # Non-blocking: never fail CI
    services:
      clickhouse:
        image: clickhouse/clickhouse-server:24.11
        ports:
          - 8123:8123
          - 9000:9000
        options: >-
          --health-cmd="wget --no-verbose --tries=1 --spider http://localhost:8123/ping || exit 1"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=5
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Install dependencies
        run: uv sync

      - name: Wait for ClickHouse
        run: |
          timeout 60 bash -c 'until wget --spider http://localhost:8123/ping; do sleep 2; done'

      - name: Initialize ClickHouse schema
        run: |
          # Create ohlcv table using schema.sql
          wget -q --post-file=schema/schema.sql http://localhost:8123/ || true

      - name: Run Arrow performance benchmarks
        id: benchmark
        continue-on-error: true  # Don't fail if benchmark script errors
        run: |
          # Run benchmark script
          uv run python benchmark_arrow_scale_analysis.py > benchmark_results.txt 2>&1 || true

          # Show results in CI logs
          if [ -f benchmark_results.txt ]; then
            echo "::group::Benchmark Results"
            cat benchmark_results.txt
            echo "::endgroup::"
          fi

      - name: Upload benchmark results
        if: always()  # Upload even if benchmark fails
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.run_id }}
          path: |
            benchmark_results.txt
            benchmark_*.csv
            benchmark_*.json
          if-no-files-found: warn
          retention-days: 30  # Keep benchmarks longer for trend analysis

      - name: Summary
        if: always()
        run: |
          echo "### ðŸ“Š Benchmark Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Benchmark job completed. This job is **informational only** and does not block CI." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ -f benchmark_results.txt ]; then
            echo "**Results:** See uploaded artifacts for detailed benchmark data." >> $GITHUB_STEP_SUMMARY
          else
            echo "**Results:** Benchmark results not generated (see logs for errors)." >> $GITHUB_STEP_SUMMARY
          fi
